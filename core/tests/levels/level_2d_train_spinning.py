#!/usr/bin/env python3
"""
LEVEL 2D: RL TRAINING - Learn to Spin to Any Angle!
=====================================================

Demonstrates all 3 RL training modes:
1. Fixed Target - Always spin to 45¬∞
2. Goal-Conditioned - Random target each episode
3. Curriculum - Progressive difficulty (15¬∞ ‚Üí 30¬∞ ‚Üí 45¬∞)

This validates our complete RL infrastructure:
- RLExperimentModal (goal-conditioned config)
- RLExperimentOps (gymnasium.Env wrapper)
- Episode boundaries on action completion (temporal abstraction!)
- Reward vectors for learning signal

For now, we'll use a simple random agent to test the infrastructure.
Later, we'll integrate Stable-Baselines3 PPO.
"""

import os
import sys
from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt

# Enable EGL for GPU-accelerated rendering
os.environ['MUJOCO_GL'] = 'egl'

sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))

from core.main.experiment_ops_unified import ExperimentOps
from core.rl.rl_experiment_modal import RLExperimentModal, TrainingMode, CurriculumLevel
from core.rl.rl_experiment_ops import RLExperimentOps


def test_fixed_target_mode():
    """Test Mode 1: Fixed Target - Always spin to 45¬∞"""
    print("=" * 70)
    print("MODE 1: FIXED TARGET - Always spin to 45¬∞")
    print("=" * 70)
    print("\nGoal: Agent learns to always spin to 45¬∞")
    print("Observation: [current_rotation, target_rotation, error]")
    print("Action: degrees to spin (-360 to +360)")
    print("Episode ends: When action completes")
    print()

    # Create experiment
    ops = ExperimentOps(mode="simulated", headless=True, render_mode="rl_core")
    ops.create_scene("rl_fixed_target", width=5, length=5, height=3)
    ops.add_robot("stretch", position=(0, 0, 0))

    # Add reward for fixed target (45¬∞)
    ops.add_reward(
        tracked_asset="stretch.base",
        behavior="rotation",
        threshold=45.0,
        reward=100.0,
        id="spin_to_45"
    )

    # Compile once (RLExperimentOps will use reset() which is fast!)
    ops.compile()

    # Define RL modal - Fixed target
    modal = RLExperimentModal(
        name="spin_to_45",
        description="Learn to always spin to 45 degrees",
        mode=TrainingMode.FIXED_TARGET,
        fixed_target=45.0,
        total_episodes=10,  # Just 10 episodes for testing
        max_steps_per_episode=300
    )

    # Create RL environment
    env = RLExperimentOps(modal, ops)

    print(f"Observation space: {env.observation_space}")
    print(f"Action space: {env.action_space}")
    print()

    # Test episodes with random agent
    episode_rewards = []
    episode_successes = []

    for episode in range(modal.total_episodes):
        obs, info = env.reset()
        episode_reward = 0

        print(f"\nEpisode {episode + 1}/{modal.total_episodes}")
        print(f"  Goal: {info['goal']:.1f}¬∞")
        print(f"  Initial obs: {obs}")

        # Random action (for testing infrastructure)
        action = env.action_space.sample()
        print(f"  Action: {action[0]:.1f}¬∞")

        # Execute action (waits for completion!)
        obs, reward, done, truncated, info = env.step(action)

        episode_reward = info['total_reward']
        episode_rewards.append(episode_reward)
        episode_successes.append(info['success'])

        print(f"  Final rotation: {info['final_rotation']:+.1f}¬∞")
        print(f"  Reward: {episode_reward:.1f}pts")
        print(f"  Success: {'‚úÖ' if info['success'] else '‚ùå'}")
        print(f"  Steps: {info['action_steps']} (action) + {info['episode_steps']} (total)")

    # Summary
    print("\n" + "=" * 70)
    print("FIXED TARGET SUMMARY")
    print("=" * 70)
    print(f"Episodes: {modal.total_episodes}")
    print(f"Success rate: {sum(episode_successes)}/{modal.total_episodes} ({100*sum(episode_successes)/modal.total_episodes:.1f}%)")
    print(f"Average reward: {np.mean(episode_rewards):.1f}pts")
    print()

    env.close()
    return episode_rewards, episode_successes


def test_goal_conditioned_mode():
    """Test Mode 2: Goal-Conditioned - Random target each episode

    NOTE: This mode requires dynamic reward changes per episode.
    Current implementation limitation: Rewards are fixed at compile time.
    TODO: Implement recompile per episode OR manual reward calculation.
    """
    print("=" * 70)
    print("MODE 2: GOAL-CONDITIONED - Random target each episode")
    print("=" * 70)
    print("\n‚ö†Ô∏è  SKIPPED: Requires dynamic rewards (not yet implemented)")
    print("\nGoal: Agent learns to spin to ANY angle (generalizable!)")
    print("Observation: [current_rotation, target_rotation, error]")
    print("Action: degrees to spin (-360 to +360)")
    print("Each episode: New random target between 10-60¬∞")
    print()

    # Return empty results for now
    return [], []

    # Create experiment
    ops = ExperimentOps(mode="simulated", headless=True, render_mode="rl_core")
    ops.create_scene("rl_goal_conditioned", width=5, length=5, height=3)
    ops.add_robot("stretch", position=(0, 0, 0))

    # Compile once
    ops.compile()

    # Define RL modal - Goal-conditioned
    modal = RLExperimentModal(
        name="spin_to_any_angle",
        description="Learn to spin to any target angle",
        mode=TrainingMode.GOAL_CONDITIONED,
        goal_range=(10.0, 60.0),  # Random targets between 10-60¬∞
        total_episodes=10,
        max_steps_per_episode=300
    )

    # Create RL environment
    env = RLExperimentOps(modal, ops)

    print(f"Observation space: {env.observation_space}")
    print(f"Action space: {env.action_space}")
    print()

    # Test episodes
    episode_rewards = []
    episode_successes = []
    episode_goals = []

    for episode in range(modal.total_episodes):
        obs, info = env.reset()

        print(f"\nEpisode {episode + 1}/{modal.total_episodes}")
        print(f"  Goal: {info['goal']:.1f}¬∞ (random!)")
        print(f"  Initial obs: {obs}")

        episode_goals.append(info['goal'])

        # Random action (for testing)
        action = env.action_space.sample()
        print(f"  Action: {action[0]:.1f}¬∞")

        obs, reward, done, truncated, info = env.step(action)

        episode_rewards.append(info['total_reward'])
        episode_successes.append(info['success'])

        print(f"  Final rotation: {info['final_rotation']:+.1f}¬∞")
        print(f"  Error: {abs(info['goal'] - info['final_rotation']):.1f}¬∞")
        print(f"  Reward: {info['total_reward']:.1f}pts")
        print(f"  Success: {'‚úÖ' if info['success'] else '‚ùå'}")

    # Summary
    print("\n" + "=" * 70)
    print("GOAL-CONDITIONED SUMMARY")
    print("=" * 70)
    print(f"Episodes: {modal.total_episodes}")
    print(f"Goal range: {modal.goal_range}")
    print(f"Goals sampled: {[f'{g:.1f}¬∞' for g in episode_goals]}")
    print(f"Success rate: {sum(episode_successes)}/{modal.total_episodes} ({100*sum(episode_successes)/modal.total_episodes:.1f}%)")
    print(f"Average reward: {np.mean(episode_rewards):.1f}pts")
    print("\nKEY INSIGHT: Agent must generalize to different goals!")
    print()

    env.close()
    return episode_rewards, episode_successes


def test_curriculum_mode():
    """Test Mode 3: Curriculum - Progressive difficulty

    NOTE: This mode also requires dynamic rewards per level.
    Current implementation limitation: Rewards are fixed at compile time.
    TODO: Implement recompile per level OR manual reward calculation.
    """
    print("=" * 70)
    print("MODE 3: CURRICULUM - Progressive difficulty (15¬∞ ‚Üí 30¬∞ ‚Üí 45¬∞)")
    print("=" * 70)
    print("\n‚ö†Ô∏è  SKIPPED: Requires dynamic rewards (not yet implemented)")
    print("\nGoal: Start easy, advance to harder targets")
    print("Observation: [current_rotation, target_rotation, error]")
    print("Action: degrees to spin (-360 to +360)")
    print("\nCurriculum levels:")
    print("  Level 1 (Easy): 10-20¬∞ targets, advance at 80% success")
    print("  Level 2 (Medium): 20-40¬∞ targets, advance at 80% success")
    print("  Level 3 (Hard): 40-60¬∞ targets")
    print()

    # Return empty results for now
    return [], []

    # Create experiment
    ops = ExperimentOps(mode="simulated", headless=True, render_mode="rl_core")
    ops.create_scene("rl_curriculum", width=5, length=5, height=3)
    ops.add_robot("stretch", position=(0, 0, 0))

    # Compile once
    ops.compile()

    # Define RL modal - Curriculum
    modal = RLExperimentModal(
        name="spin_curriculum",
        description="Progressive difficulty curriculum",
        mode=TrainingMode.CURRICULUM,
        curriculum=[
            CurriculumLevel(
                name="easy",
                target_range=(10, 20),
                success_threshold=0.8,
                min_episodes=5  # Just 5 for testing
            ),
            CurriculumLevel(
                name="medium",
                target_range=(20, 40),
                success_threshold=0.8,
                min_episodes=5
            ),
            CurriculumLevel(
                name="hard",
                target_range=(40, 60),
                success_threshold=0.8,
                min_episodes=5
            )
        ],
        total_episodes=20,  # Should advance through levels
        max_steps_per_episode=300
    )

    # Create RL environment
    env = RLExperimentOps(modal, ops)

    print(f"Observation space: {env.observation_space}")
    print(f"Action space: {env.action_space}")
    print()

    # Test episodes
    episode_rewards = []
    episode_successes = []
    episode_levels = []

    for episode in range(modal.total_episodes):
        obs, info = env.reset()

        print(f"\nEpisode {episode + 1}/{modal.total_episodes} [Level: {info['level']}]")
        print(f"  Goal: {info['goal']:.1f}¬∞")

        # Random action
        action = env.action_space.sample()
        print(f"  Action: {action[0]:.1f}¬∞")

        obs, reward, done, truncated, info = env.step(action)

        episode_rewards.append(info['total_reward'])
        episode_successes.append(info['success'])
        episode_levels.append(info['level'])

        print(f"  Final rotation: {info['final_rotation']:+.1f}¬∞")
        print(f"  Reward: {info['total_reward']:.1f}pts")
        print(f"  Success: {'‚úÖ' if info['success'] else '‚ùå'}")

        if info['level_advanced']:
            print(f"  üéâ ADVANCED TO NEXT LEVEL: {info['level']}!")

    # Summary
    print("\n" + "=" * 70)
    print("CURRICULUM SUMMARY")
    print("=" * 70)
    print(f"Episodes: {modal.total_episodes}")
    print(f"Final level: {modal.get_current_level_name()}")
    print(f"Success rate: {sum(episode_successes)}/{modal.total_episodes} ({100*sum(episode_successes)/modal.total_episodes:.1f}%)")
    print(f"Average reward: {np.mean(episode_rewards):.1f}pts")
    print("\nKEY INSIGHT: Curriculum learning helps with complex tasks!")
    print()

    env.close()
    return episode_rewards, episode_successes


def main():
    """Run all 3 RL training modes"""
    print("=" * 70)
    print("LEVEL 2D: RL TRAINING - Learn to Spin to Any Angle!")
    print("=" * 70)
    print("\nThis test validates our complete RL infrastructure:")
    print("  ‚úì RLExperimentModal (goal-conditioned config)")
    print("  ‚úì RLExperimentOps (gymnasium.Env wrapper)")
    print("  ‚úì Episode boundaries on action completion")
    print("  ‚úì Reward vectors for learning signal")
    print("\nWe test 3 modes with RANDOM agent (to validate infrastructure):")
    print()

    # Test all 3 modes
    print("\n" + "‚îÄ" * 70 + "\n")
    fixed_rewards, fixed_success = test_fixed_target_mode()

    print("\n" + "‚îÄ" * 70 + "\n")
    goal_rewards, goal_success = test_goal_conditioned_mode()

    print("\n" + "‚îÄ" * 70 + "\n")
    curriculum_rewards, curriculum_success = test_curriculum_mode()

    # Final summary
    print("\n" + "=" * 70)
    print("FINAL SUMMARY - Modes Tested")
    print("=" * 70)
    print("\n Mode                Status          Success Rate    Avg Reward")
    print("‚îÄ" * 70)
    if fixed_success:
        print(f" Fixed Target        ‚úÖ Tested       {100*sum(fixed_success)/len(fixed_success):5.1f}%         {np.mean(fixed_rewards):6.1f}pts")
    else:
        print(f" Fixed Target        ‚ùå Error")

    if goal_success:
        print(f" Goal-Conditioned    ‚úÖ Tested       {100*sum(goal_success)/len(goal_success):5.1f}%         {np.mean(goal_rewards):6.1f}pts")
    else:
        print(f" Goal-Conditioned    ‚ö†Ô∏è  Skipped     (dynamic rewards needed)")

    if curriculum_success:
        print(f" Curriculum          ‚úÖ Tested       {100*sum(curriculum_success)/len(curriculum_success):5.1f}%         {np.mean(curriculum_rewards):6.1f}pts")
    else:
        print(f" Curriculum          ‚ö†Ô∏è  Skipped     (dynamic rewards needed)")
    print("\n" + "=" * 70)
    print("\n‚úÖ RL INFRASTRUCTURE VALIDATED!")
    print("\nNext steps:")
    print("  1. Integrate Stable-Baselines3 PPO")
    print("  2. Train actual RL agent (not random)")
    print("  3. Visualize learning curves")
    print("  4. Test on real robot!")
    print()


if __name__ == "__main__":
    main()
